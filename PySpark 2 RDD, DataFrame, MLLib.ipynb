{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(os.path.join('..', 'data', 'ml-100k', 'u.item')) as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "            \n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseInput(line):\n",
    "    fields = line.split()\n",
    "    # fields[1] : movieID, fields[2]: rating\n",
    "    return (int(fields[1]), (float(fields[2]), 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    conf = SparkConf().setAppName(\"WorstMovies\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    \n",
    "    # load up our movie ID -> movie name lookup table\n",
    "    movieNames = loadMovieNames()\n",
    "    \n",
    "    # load ut the raw u.data file\n",
    "    lines = sc.textFile(os.path.join('..', 'data', 'ml-100k', 'u.data'))\n",
    "    # lines = sc.textFile(os.path.join('hdfs://', 'user', 'maria_dev', 'ml-100k', 'u.item'))\n",
    "    \n",
    "    # convert to (movieID, (rating, 1.0))\n",
    "    movieRatings = lines.map(parseInput)\n",
    "    \n",
    "    # reduce to (moiveID), (sumOfRatings, totalRatings)\n",
    "    ratingTotalsAndCount = movieRatings.reduceByKey(lambda movie1, movie2: (movie1[0] + movie2[0]))\n",
    "    \n",
    "    # map to (movieID, averageRating)\n",
    "    averageRatings = ratingTotalsAndCount.mapValues(lambda totalAndCount : totalAndCount[0] / totalAndCount[1])\n",
    "    \n",
    "    # sort by average rating\n",
    "    sortedMovies = averageRatings.sortBy(lambda x: x[1])\n",
    "    \n",
    "    # take the top 10 results\n",
    "    results = sortedMovies.take(10)\n",
    "    \n",
    "    # print them out\n",
    "    for result in results:\n",
    "        print(movieNames[result[0]], result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(os.path.join('..', 'data', 'ml-100k', 'u.item')) as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "            \n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseInput(line):\n",
    "    fields = line.split()\n",
    "    \n",
    "    return Row(movieID=int(fields[1]), rating=float(fields[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# load up our movie ID -> name dictionary\n",
    "movieNames = loadMoiveNames()\n",
    "\n",
    "# get the raw data\n",
    "lines = spark.sparkContext.textFile(os.path.join('..', 'data', 'ml-100k', 'u.data'))\n",
    "# lines = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
    "\n",
    "# convert it to a RDD of Row objects with (movieID, rating)\n",
    "movies = lines.map(parseInput)\n",
    "\n",
    "# convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "# compute average rating for each movieID\n",
    "averageRatings = movieDataset.groupBy(\"movieID\").avg(\"rating\")\n",
    "\n",
    "# compute count of ratings for each movieID\n",
    "counts = movieDataset.groupby(\"movieID\").count()\n",
    "\n",
    "# join the two dataset\n",
    "averageAndCounts = counts.join(averageRatings, \"movieID\")\n",
    "\n",
    "# pull the top 10 results\n",
    "topTen = averageAndCounts.orderBy(\"avg(rating)\").take(10)\n",
    "\n",
    "# print result\n",
    "for movie in topTen:\n",
    "    print(movieNames[movie[0]], movie[1], movie[2])\n",
    "    \n",
    "# stop the session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Recommendation with MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up movieID -> movie name dictionary\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(os.path.join('..', 'data', 'ml-100k', 'u.item')) as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "            \n",
    "    return movieNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert u.data lines into (userID, movieID, rating) rows\n",
    "def parseInput(line):\n",
    "    fields = line.value.split()\n",
    "    \n",
    "    return Row(userID=int(fields[0]), movieID=int(fields[1]), rating=float(fields[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"movieRecs\").getOrCreate()\n",
    "\n",
    "# load up our movie ID -> name dictionary\n",
    "movieNames = loadMoiveNames()\n",
    "\n",
    "# get the raw data\n",
    "lines = spark.sparkContext.textFile(os.path.join('..', 'data', 'ml-100k', 'u.data'))\n",
    "# lines = spark.sparkContext.textFile(\"hdfs:///user/maria_dev/ml-100k/u.data\")\n",
    "\n",
    "# convert it to a RDD of Row objects with (userID, movieID, rating)\n",
    "ratingsRDD = lines.map(parseInput)\n",
    "\n",
    "# convert to a DataFrame and cache it\n",
    "ratings = spark.createDataFrame(ratingsRDD).cashe()\n",
    "\n",
    "# creat an ALS collaborative filtering modle from the complete dataset\n",
    "als = ALS(maxIter=5, regParma=0.01, userCol='userID', itemCol='movieID', ratingCol='rating')\n",
    "model = als.fit(ratings)\n",
    "\n",
    "# print out ratings from user 0:\n",
    "print(\"\\nRatings for user ID 0:\")\n",
    "userRatings = ratings.filter(\"userID = 0\")\n",
    "for rating in userRatings.collect():\n",
    "    print(moiveNames[rating['movieID']], rating['rating'])\n",
    "    \n",
    "print(\"\\nTop 20 recommendations:\")\n",
    "# find movies rated more than 100 times:\n",
    "ratingCounts = ratings.groupby(\"movieID\").count().filter(\"count > 100\")\n",
    "\n",
    "# construct a test dataframe for user 0 with every movie rated more than 100 times\n",
    "popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(0))\n",
    "\n",
    "# run model on that list of popular movies for user ID 0\n",
    "recommendations = model.transform(popularMovies)\n",
    "\n",
    "# get the top 20 movies with the highest predicted rating for this user\n",
    "topRecommendations = recommendations.sort(recommendations.prediction.desc()).take(20)\n",
    "\n",
    "for recommendation in topRecommendations:\n",
    "    print(movieNames[recommendation['movieID']], recommendation['prediction'])\n",
    "    \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
